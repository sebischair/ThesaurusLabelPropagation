{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_file = \"<PROJECT_DIR>/output/02_word_embeddings/<DATE>/main.txt\"\n",
    "df_embeddings = pd.read_table(\n",
    "        embeddings_file,\n",
    "        skiprows=1,\n",
    "        sep=\" \",\n",
    "        header=None,\n",
    "        index_col=0)\n",
    "df_embeddings.index.names = ['word']\n",
    "df_embeddings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_file_lp = \"<PROJECT_DIR>/output/08_propagation_evaluation/<DATE>/main.txt\"\n",
    "evaluation_file_baseline = \"<PATH_TO_BASELINE_DF_EVALUATION>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_synsets = [15456] \n",
    "# chosen_synsets = [9444]\n",
    "topx = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lp = pd.read_csv(\n",
    "    evaluation_file_lp,\n",
    "    sep=\" \",\n",
    "    index_col=0)\n",
    "df_lp = df_lp.sort_values(by=\"y_conf\", ascending=False)\n",
    "df_lp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_baseline = pd.read_csv(\n",
    "    evaluation_file_baseline,\n",
    "    sep=\" \",\n",
    "    index_col=0)\n",
    "df_baseline = df_baseline.sort_values(by=\"y_conf\", ascending=False)\n",
    "df_baseline.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_words_lp = df_lp.loc[df_lp['y_train'].isin(chosen_synsets)][\"y_train\"]\n",
    "chosen_words_baseline = df_baseline.loc[df_baseline['y_train'].isin(chosen_synsets)][\"y_train\"]\n",
    "\n",
    "for synset in chosen_synsets:\n",
    "    chosen_words_lp = pd.concat([chosen_words_lp, df_lp.loc[(df_lp['y_train'] != synset) & (df_lp['y_pred']== synset)][\"y_pred\"].head(topx)])\n",
    "    chosen_words_baseline = pd.concat([chosen_words_baseline, df_baseline.loc[(df_baseline['y_train'] != synset) & (df_baseline['y_pred']==synset)][\"y_pred\"].head(topx)])\n",
    "\n",
    "\n",
    "print(len(chosen_words_lp))\n",
    "print(len(chosen_words_baseline))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_words_lp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lp.loc[df_lp['y_train'].isin(chosen_synsets)][\"y_train\"].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_lp = chosen_words_lp.index.to_frame().reset_index(drop=True)\n",
    "words_baseline = chosen_words_baseline.index.to_frame().reset_index(drop=True)\n",
    "\n",
    "words_merged = words_lp.merge(words_baseline, how=\"outer\", indicator=True)\n",
    "is_training = words_merged[\"word\"].isin(df_lp.loc[df_lp['y_train'].isin(chosen_synsets)][\"y_train\"].index.tolist())\n",
    "words_merged.loc[\n",
    "    is_training, \n",
    "    'merge'] = \"training\"\n",
    "words_merged.loc[\n",
    "    ~is_training, \n",
    "    'merge'] = words_merged[\"_merge\"]\n",
    "words_merged = words_merged[[\"word\", \"merge\"]]\n",
    "words_merged = words_merged.set_index(\"word\")\n",
    "print(len(words_merged))\n",
    "words_merged = words_merged.sort_values(by=\"merge\")\n",
    "words_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_embeddings = df_embeddings[df_embeddings.index.isin(words_merged.index.tolist())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_list = words_merged.join(chosen_embeddings)\n",
    "full_list.iloc[:,0:400].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_without_umlaut = [item.replace(u'ü', 'ue').replace(u'ö', 'oe').replace(u'ä', 'ae').replace(u'ß', 'ss') for item in full_list.index.tolist()]\n",
    "full_list.index = index_without_umlaut\n",
    "full_list.iloc[:,1:401].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export embeddings\n",
    "full_list.iloc[:,1:401].to_csv(\"tensorboard_embeddings_thesis_synset_detailed_combined.tsv\", sep=\"\\t\", header = False, index = False)\n",
    "# export synsets & words\n",
    "full_list.iloc[:,0].to_csv(\"tensorboard_labels_without_umlauts_thesis_synset_detailed_combined.tsv\", sep=\"\\t\", header = True, index = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
