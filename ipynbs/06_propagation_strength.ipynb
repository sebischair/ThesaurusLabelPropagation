{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_df_evaluation_just_test_data_output(file_path):\n",
    "    df = pd.read_csv(\n",
    "    file_path,\n",
    "    sep=\" \",\n",
    "    index_col=0)\n",
    "    \n",
    "    df = df.fillna(value={\"y_test\": -1})\n",
    "\n",
    "    if len(df[(df.y_train == -1) & (df.y_test != -1) & (df.y_pred == -1)]) != 0:\n",
    "        print(\"WARNING, there are test rows without predictions:\")\n",
    "        print(df[(df.y_train == -1) & (df.y_test != -1) & (df.y_pred == -1)].head())\n",
    "        print(len(df[(df.y_train == -1) & (df.y_test != -1) & (df.y_pred == -1)]))\n",
    "    else:\n",
    "        print(\"All right\")\n",
    "\n",
    "    df_evaluation = df.sort_index()\n",
    "    \n",
    "    # IMPORTANT: JUST LOAD THE TEST DATA ROWS!\n",
    "    df_evaluation = df_evaluation[df_evaluation.y_test != -1]\n",
    "\n",
    "    df_evaluation = df_evaluation[[\"y_pred\", \"y_conf\", \"y_test\"]]\n",
    "    \n",
    "    return df_evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "\n",
    "def similarity_matrix(_datasets):\n",
    "    load = load_df_evaluation_just_test_data_output\n",
    "    \n",
    "    loaded_datasets = [{ \n",
    "        \"name\": _dataset[\"name\"], \n",
    "        \"df_eval\": load(_dataset[\"path\"])\n",
    "    } for _dataset in _datasets]\n",
    "    \n",
    "    synsets = pd.read_csv(\n",
    "        _datasets[0][\"path\"],\n",
    "        sep=\" \",\n",
    "        index_col=0).y_train.unique()\n",
    "    \n",
    "    results = []\n",
    "    for a, b in itertools.combinations(loaded_datasets, r=2):        \n",
    "        # group by synset and sort desc by conf\n",
    "        a_groups = a[\"df_eval\"].reset_index().groupby([\"y_pred\"]).apply(\n",
    "            lambda x: x.sort_values([\"y_conf\"], ascending=False)\n",
    "        ).groupby(level=0).head(10).groupby(level=0)\n",
    "        \n",
    "        b_groups = b[\"df_eval\"].reset_index().groupby([\"y_pred\"]).apply(\n",
    "            lambda x: x.sort_values([\"y_conf\"], ascending=False)\n",
    "        ).groupby(level=0).head(10).groupby(level=0)\n",
    "        \n",
    "        # Intersect the respective synsets and get mean intersection\n",
    "        intersections = []\n",
    "        equals = []\n",
    "        for synset in synsets:\n",
    "            if (synset not in a_groups.groups) or (synset not in b_groups.groups):\n",
    "                intersections.append(0)\n",
    "                continue\n",
    "\n",
    "            a_words = a_groups.get_group(synset).word.tolist()\n",
    "            b_words = b_groups.get_group(synset).word.tolist()\n",
    "\n",
    "            intersections.append(len(\n",
    "                set(a_words).intersection(\n",
    "                    b_words)\n",
    "            )/np.max([len(a_words), len(b_words)]))\n",
    "            equals.append(len(\n",
    "                set(a_words).intersection(\n",
    "                    b_words)\n",
    "            ))\n",
    "\n",
    "        print((a[\"name\"], b[\"name\"]), \"intersections\", np.mean(intersections), \"equals\", np.sum(equals))\n",
    "        results.append({\n",
    "            \"combination\": (a[\"name\"], b[\"name\"]),\n",
    "            \"intersections\": intersections,\n",
    "            \"equals\": equals\n",
    "        })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When using 50% test data => how many of the matched test data are similar & what would the combined accuracy be?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# using sampling with rng_num=1\n",
    "ft_lp = {\n",
    "    \"name\": \"ft_lp\",\n",
    "    \"path\": \"<PROJECT_DIR>/08_propagation_evaluation/20181029-150442/main.txt\" \n",
    "}\n",
    "\n",
    "w2v_lp = {\n",
    "    \"name\": \"w2v_lp\",\n",
    "    \"path\": \"<PROJECT_DIR>/08_propagation_evaluation/20181029-152409/main.txt\" \n",
    "}\n",
    "\n",
    "ft_baseline_k200 = {\n",
    "    \"name\": \"ft_baseline_k200\",\n",
    "    \"path\": \"<DF_EVALUATION_PATH>\"\n",
    "}\n",
    "\n",
    "w2v_baseline_k200 = {\n",
    "    \"name\": \"w2v_baseline_k200\",\n",
    "    \"path\": \"<DF_EVALUATION_PATH>\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def pairwise_combined_accuracy(_datasets):\n",
    "    loaded_datasets = [{ \n",
    "        \"name\": _dataset[\"name\"], \n",
    "        \"df_eval\": load_df_evaluation_just_test_data_output(_dataset[\"path\"])\n",
    "    } for _dataset in _datasets]\n",
    "    \n",
    "    for d in loaded_datasets:\n",
    "        df = d[\"df_eval\"]\n",
    "        if len(df[df.y_test == -1]) > 0:\n",
    "            print(\"All y_test should have a value!\")\n",
    "        \n",
    "        accuracy = len(df[df.y_pred == df.y_test]) / len(df)\n",
    "        print(d[\"name\"], \"test accuracy: \", accuracy)\n",
    "        \n",
    "    for a, b in itertools.combinations(loaded_datasets, r=2): \n",
    "        combined = a[\"df_eval\"].join(b[\"df_eval\"], lsuffix=\"_a\", rsuffix=\"_b\")[[\"y_pred_a\", \"y_test_a\", \"y_pred_b\"]]\n",
    "        print()\n",
    "        print()\n",
    "        print(a[\"name\"], b[\"name\"])\n",
    "        print(\"Equal test predictions\", len(combined[(combined.y_pred_a != -1) & (combined.y_pred_b != -1) & (combined.y_pred_a == combined.y_pred_b)])/len(combined))\n",
    "        print(\"Combined accuracies\", len(combined[(combined.y_pred_a == combined.y_test_a) | (combined.y_pred_b == combined.y_test_a)])/len(combined))\n",
    "\n",
    "        \n",
    "datasets = [ft_lp, w2v_lp, ft_baseline_k200, w2v_baseline_k200]      \n",
    "# similarity_matrix(datasets)\n",
    "pairwise_combined_accuracy(datasets)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
